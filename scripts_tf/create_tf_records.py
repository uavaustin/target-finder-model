"""
AFTER running create_full_images.py and create_detection_data.py:

$ python scripts_tf/create_tf_records.py --image_dir ./scripts_generate/data
"""
import argparse
import random
import pathlib
import json

import contextlib2
from PIL import Image
import glob
from io import BytesIO

import tensorflow as tf

from object_detection.dataset_tools import tf_record_creation_util
from object_detection.utils import dataset_util

with pathlib.Path("config.yaml").open("r") as stream:
    import yaml

    config = yaml.safe_load(stream)

DET_CLASSES = config["classes"]["shapes"] + config["classes"]["alphas"]
CLF_CLASSES = config["classes"]["types"]
FORMAT = config["generate"]["img_ext"]


def parse_annotation_data(data):
    labels = []
    data = data.replace("\r", "").strip()
    for line in data.split("\n"):
        labels.append([float(val) for val in line.split(" ")])
    return labels


def create_tf_example(filename: pathlib.Path):
    """Converts image and txt annotations to a tf.Example proto.
    """
    image_path_prefix = filename.with_suffix("")

    img_encoded = filename.read_bytes()
    image_width, image_height = Image.open(BytesIO(img_encoded)).size

    annotations = []
    has_target = 1

    if filename.with_suffix(".txt").is_file():
        # For object detection
        with filename.with_suffix(".txt").open("r") as annotations_fp:
            annotations = parse_annotation_data(annotations_fp.read())
    else:
        # For clf
        has_target = 0 if "target" not in image_path_prefix.name else 1

    xmin = []
    xmax = []
    ymin = []
    ymax = []
    category_names = []
    category_ids = []

    for idx, (obj_id, x_n, y_n, w_n, h_n) in enumerate(annotations):

        xmin.append(x_n)
        xmax.append(x_n + w_n)
        ymin.append(y_n)
        ymax.append(y_n + h_n)

        category_id = int(obj_id)
        category_ids.append(category_id + 1)
        category_names.append(DET_CLASSES[category_id].encode("utf8"))

    feature_dict = {
        "image/height": dataset_util.int64_feature(image_height),
        "image/width": dataset_util.int64_feature(image_width),
        "image/filename": dataset_util.bytes_feature(str(filename).encode("utf8")),
        "image/source_id": dataset_util.bytes_feature(
            str(image_path_prefix.name).encode("utf8")
        ),
        "image/encoded": dataset_util.bytes_feature(img_encoded),
        "image/colorspace": dataset_util.bytes_feature("RGB".encode("utf8")),
        "image/format": dataset_util.bytes_feature(FORMAT.encode("utf8")),
        "image/object/bbox/xmin": dataset_util.float_list_feature(xmin),
        "image/object/bbox/xmax": dataset_util.float_list_feature(xmax),
        "image/object/bbox/ymin": dataset_util.float_list_feature(ymin),
        "image/object/bbox/ymax": dataset_util.float_list_feature(ymax),
        "image/object/class/text": dataset_util.bytes_list_feature(category_names),
        "image/object/class/label": dataset_util.int64_list_feature(category_ids),
        "image/class/label": dataset_util.int64_feature(has_target),
        "image/class/text": dataset_util.bytes_feature(
            CLF_CLASSES[has_target].encode("utf8")
        ),
    }
    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))

    return example


def _create_tf_record_from_images(data_dir: pathlib.Path, output_path: pathlib.Path):
    """Loads images generated by generate/*.py scripts and converts
    them into tf records.
    """
    # Determine number of shards. Recommended ~2000 images per shard
    image_fns = list(pathlib.Path(data_dir).rglob(f"*.{FORMAT}"))
    random.shuffle(image_fns)

    num_shards = len(image_fns) // 2000 + 1

    with contextlib2.ExitStack() as tf_record_close_stack:

        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
            tf_record_close_stack, output_path, num_shards
        )

        for idx, image_fn in enumerate(image_fns):
            if idx % 1000 == 0:
                print(f"On image {idx} of {len(image_fns)}")

            tf_example = create_tf_example(image_fn)
            shard_idx = idx % num_shards
            output_tfrecords[shard_idx].write(tf_example.SerializeToString())


def main(
    image_dir: pathlib.Path,
    output_dir: pathlib.Path,
    detection: bool,
    classification: bool,
):

    output_dir.mkdir(exist_ok=True, parents=True)

    if detection:

        _create_tf_record_from_images(
            image_dir / "detector_train" / "images", output_dir / "tfm_train.record",
        )

        _create_tf_record_from_images(
            image_dir / "detector_val" / "images", output_dir / "tfm_val.record",
        )

    if classification:
        _create_tf_record_from_images(
            image_dir / "clf_train", output_dir / "tfm_clf_train.record",
        )

        _create_tf_record_from_images(
            image_dir / "clf_val", output_dir / "tfm_clf_val.record",
        )


if __name__ == "__main__":

    random.seed(42)

    parser = argparse.ArgumentParser(
        description="Creates TF-Records for training, eval."
    )
    parser.add_argument(
        "--image_dir",
        type=pathlib.Path,
        required=False,
        default="scripts_generate/data",
    )
    parser.add_argument(
        "--output_dir", type=pathlib.Path, required=False, default="model_data/records"
    )
    parser.add_argument("--detection", action="store_true", default=True)
    parser.add_argument("--classification", action="store_true", default=True)
    args = parser.parse_args()

    main(
        args.image_dir.expanduser(),
        args.output_dir.expanduser(),
        args.classification,
        args.detection,
    )
