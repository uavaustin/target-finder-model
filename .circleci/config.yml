defaults: &defaults
  docker:
    - image: circleci/python:3.6

version: 2
jobs:
  # Testing job which builds a small model and runs sanity tests
  # against it.
  test:
    <<: *defaults
    steps:
      - checkout
      - run: pip install --user tox
      - run: python -m tox
      - persist_to_workspace:
          root: generate
          paths:
            - assets
      - run: tar -cvzf test-data.tar.gz generate/data
      - store_artifacts:
          path: test-data.tar.gz
          destination: test-data.tar.gz

  # Parallel jobs to build shapes.
  part-a-gen:
    <<: *defaults
    environment:
      - NUM_OFFSET: 0
      - NUM_IMAGES: 10
      - NUM_VAL_OFFSET: 0
      - NUM_VAL_IMAGES: 10
    steps:
      - checkout
      - attach_workspace:
          at: generate
      - run: pip install --user -r requirements-dev.txt
      - run: python generate/create_full_images.py
      - run: python generate/create_clf_data.py
      - run: python generate/create_detection_data.py
      - run: mv generate/data/detector_train/images/detector_train_list.txt generate/data/detector_train/images/detector_train_list_a.txt
      - run: mv generate/data/detector_val/images/detector_val_list.txt generate/data/detector_val/images/detector_val_list_a.txt
      - run: mv generate/data/clf_train/images/clf_train_list.txt generate/data/clf_train/images/clf_train_list_a.txt
      - run: mv generate/data/clf_val/images/clf_val_list.txt generate/data/clf_val/images/clf_val_list_a.txt
      - persist_to_workspace:
          root: generate
          paths:
            - data/*
  part-b-gen:
    <<: *defaults
    environment:
      - NUM_OFFSET: 10
      - NUM_IMAGES: 10
      - NUM_VAL_OFFSET: 10
      - NUM_VAL_IMAGES: 10
    steps:
      - checkout
      - attach_workspace:
          at: generate
      - run: pip install --user -r requirements-dev.txt
      - run: python generate/create_full_images.py
      - run: python generate/create_clf_data.py
      - run: python generate/create_detection_data.py
      - run: mv generate/data/detector_train/images/detector_train_list.txt generate/data/detector_train/images/detector_train_list_b.txt
      - run: mv generate/data/detector_val/images/detector_val_list.txt generate/data/detector_val/images/detector_val_list_b.txt
      - run: mv generate/data/clf_train/images/clf_train_list.txt generate/data/clf_train/images/clf_train_list_b.txt
      - run: mv generate/data/clf_val/images/clf_val_list.txt generate/data/clf_val/images/clf_val_list_b.txt
      - persist_to_workspace:
          root: generate
          paths:
            - data/*
  part-c-gen:
    <<: *defaults
    environment:
      - NUM_OFFSET: 20
      - NUM_IMAGES: 10
      - NUM_VAL_OFFSET: 20
      - NUM_VAL_IMAGES: 10
    steps:
      - checkout
      - attach_workspace:
          at: generate
      - run: pip install --user -r requirements-dev.txt
      - run: python generate/create_full_images.py
      - run: python generate/create_clf_data.py
      - run: python generate/create_detection_data.py
      - run: mv generate/data/detector_train/images/detector_train_list.txt generate/data/detector_train/images/detector_train_list_c.txt
      - run: mv generate/data/detector_val/images/detector_val_list.txt generate/data/detector_val/images/detector_val_list_c.txt
      - run: mv generate/data/clf_train/images/clf_train_list.txt generate/data/clf_train/images/clf_train_list_c.txt
      - run: mv generate/data/clf_val/images/clf_val_list.txt generate/data/clf_val/images/clf_val_list_c.txt
      - persist_to_workspace:
          root: generate
          paths:
            - data/*
  part-d-gen:
    <<: *defaults
    environment:
      - NUM_OFFSET: 30
      - NUM_IMAGES: 10
      - NUM_VAL_OFFSET: 30
      - NUM_VAL_IMAGES: 10
    steps:
      - checkout
      - attach_workspace:
          at: generate
      - run: pip install --user -r requirements-dev.txt
      - run: python generate/create_full_images.py
      - run: python generate/create_clf_data.py
      - run: python generate/create_detection_data.py
      - run: mv generate/data/detector_train/images/detector_train_list.txt generate/data/detector_train/images/detector_train_list_d.txt
      - run: mv generate/data/detector_val/images/detector_val_list.txt generate/data/detector_val/images/detector_val_list_d.txt
      - run: mv generate/data/clf_train/images/clf_train_list.txt generate/data/clf_train/images/clf_train_list_d.txt
      - run: mv generate/data/clf_val/images/clf_val_list.txt generate/data/clf_val/images/clf_val_list_d.txt
      - persist_to_workspace:
          root: generate
          paths:
            - data/*

  merge-data:
    <<: *defaults
    steps:
      - checkout
      - attach_workspace:
          at: generate
      - run: cat generate/data/detector_train/images/detector_train_list_*.txt > generate/data/detector_train/images/detector_train_list.txt
      - run: cat generate/data/detector_val/images/detector_val_list_*.txt > generate/data/detector_val/images/detector_val_list.txt
      - run: cat generate/data/clf_train/images/clf_train_list_*.txt > generate/data/clf_train/images/clf_train_list.txt
      - run: cat generate/data/clf_val/images/clf_val_list_*.txt > generate/data/clf_val/images/clf_val_list.txt
      - persist_to_workspace:
          root: generate
          paths:
            - data/*

  setup-darknet:
    <<: *defaults
    steps:
      - checkout
      - attach_workspace:
          at: '..'
      - run: git clone https://github.com/AlexeyAB/darknet.git
      - run: make -C ./darknet
      - run: wget -P darknet https://pjreddie.com/media/files/darknet53.conv.74
      - persist_to_workspace:
          root: 'darknet'
          paths:
            - 'darknet'
            - 'darknet53.conv.74'

  # Training job using the shapes from the shapes above.
  train: &image-train-job
    <<: *defaults
    steps:
      - checkout
      - attach_workspace:
          at: 'darknet'
      - attach_workspace:
          at: 'generate'
      - run: ls
      - run: ls darknet
      - run: ls generate/data
      - run: cat generate/data/detector_train/images/detector_train_list.txt
      - run:
          name: DarknetDetector
          command: darknet/darknet detector train model/cfgs/yolo3detector.data model/cfgs/yolo3detector-train.cfg darknet/darknet53.conv.74
          no_output_timeout: 10m
      - persist_to_workspace:
          root: model
          paths:
            - weights/*
  train-clf:
    <<: *defaults
    steps:
      - checkout
      - attach_workspace:
          at: 'darknet'
      - attach_workspace:
          at: 'generate'
      - run: cat generate/data/clf_train/images/clf_train_list.txt
      - run:
          name: DarknetClf
          command: darknet/darknet classifier train model/cfgs/preclf.data model/cfgs/preclf-train.cfg
          no_output_timeout: 10m
      - persist_to_workspace:
          root: model
          paths:
            - weights/*

  # Job that creates a tar archive for the python library.
  release:
    <<: *defaults
    steps:
      - checkout
      - attach_workspace:
          at: '.'
      - run: ./scripts/create-release.sh
      - store_artifacts:
          path: release
          destination: release
      - persist_to_workspace:
          root: '.'
          paths:
            - release

  # Job that publishes to GitHub Releases for tags.
  publish:
    docker:
      - image: cibuilds/github:0.12
    steps:
      - attach_workspace:
          at: '.'
      - run: |
          ghr \
            -t ${GITHUB_TOKEN} \
            -u ${CIRCLE_PROJECT_USERNAME} \
            -r ${CIRCLE_PROJECT_REPONAME} \
            -c ${CIRCLE_SHA1} \
            -delete \
            ${CIRCLE_TAG} \
            ./release/

workflows:
  version: 2
  test-generate-train:
    jobs:
      - test
      - setup-darknet
      - part-a-gen: &gen-job
          requires:
            - test
      - part-b-gen:
          <<: *gen-job
      - part-c-gen:
          <<: *gen-job
      - part-d-gen:
          <<: *gen-job
      - merge-data:
          requires:
            - part-a-gen
            - part-b-gen
            - part-c-gen
            - part-d-gen
      - train: &train-job
          requires:
            - merge-data
            - setup-darknet
      - train-clf:
          <<: *train-job
      - release:
          requires:
            - train
            - train-clf
